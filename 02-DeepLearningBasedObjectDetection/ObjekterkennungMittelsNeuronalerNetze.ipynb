{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a0d8ab0-c23c-4907-a1ed-e5cfbfd29cf4",
   "metadata": {},
   "source": [
    "# Einheit 2 - Moderne Objekterkennung\n",
    "\n",
    "## Inhalt der Einheit: Einführung in die Objekterkennung\n",
    "- Faltungsneuronale Netzwerke - Moderne Objekterkennung\n",
    "- VGG16\n",
    "- Resnet\n",
    "- MaskRCNN\n",
    "- CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab160cd-7222-403f-9d86-71e83c22e15a",
   "metadata": {},
   "source": [
    "## Faltungsneuronale Netzwerke - Moderne Objekterkennung\n",
    "\n",
    "In dieser Einheit bauen wir auf die Ergebnisse der letzten Einheit auf, in welcher wir durch die Bewegung von Filtern eine Gesichtserkennung gebaut haben (Abschnitte Erweiterte Objekterkennung und Live Gesichterkennung mittels Filter). Hier verwenden wir so genannte Faltungsneuronale Netzwerke (CNNs), mit welchen diese Filter automatisch berechnet werden! Ihr müsst euch also um nichts kümmern, das Netzwerk findet automatisch heraus, wie es gut Objekte in Bildern erkennen kann.\n",
    "\n",
    "Hierfür müssen wir als erstes neue Module installieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eba561-61a7-41bf-9016-d54f32eeaf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation der neuen Module (wir verwenden Pytorch)\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "%pip install ftfy regex tqdm\n",
    "%pip install git+https://github.com/openai/CLIP.git\n",
    "%pip install scikit-image\n",
    "%pip install pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8385cf-dd5f-485d-9f17-c9f90f766b6e",
   "metadata": {},
   "source": [
    "## Objekterkennung mit tiefen Neuronalen Netzwerken (VGG16)\n",
    "\n",
    "VGG16 ist ein relativ altes (2014) Neuronales Netzwerk für die Objekterkennung in Bildern. Wir verwenden VGG um diese Objekte https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt automatisch in Bildern zu erkennen. Im Grunde ist unser Aufbau der gleiche wie letzte Einheit. Wir laden unsere module mittels:\n",
    "```\n",
    "import cv2\n",
    "```\n",
    "\n",
    "Danach starten wir die Kamera mit:\n",
    "```\n",
    "camera = cv2.VideoCapture(0)\n",
    "```\n",
    "\n",
    "Um nun Objekte in unserem Frame zu erkennen, müssen wir unseren Objekterkenner erstellen. Ich habe euch den Programmiercode hierfür vereinfacht. Ihr könnt den Objekterkenner so erstellen:\n",
    "```\n",
    "model = dlmodels.VGG16ObjectDetector() \n",
    "```\n",
    "\n",
    "Für mehr Information über den Programmiercode des Objekterkenners könnt ihr im dlmodels.py nachschauen. Darauf folgend starten wir die Endlosschleife und laden den aktuellen Frame mittels:\n",
    "```\n",
    "while True:\n",
    "    error, frame = camera.read()\n",
    "    if not error:\n",
    "        break\n",
    "```\n",
    "\n",
    "Jetzt können wir die Objekte in einem frame über model.predict vorhersagen. Diese sind nicht perfekt, gehen aber schon in die richtige Richtung:\n",
    "``` \n",
    "    labels = model.predict(frame)\n",
    "```\n",
    "\n",
    "Abschließend schreiben wir Text auf unser Bild welches die am warscheindlichste Vorhersage ausgibt, geben den Frame aus und ermöglichen die Beendung der Aufnahme wenn die Taste q gedrückt wird. Nach der Endlosschleife geben wir die Kamera frei und schließen alle Fenster.\n",
    "\n",
    "```\n",
    "    cv2.putText(frame, labels[0], (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Und zeigen die Kamera an\n",
    "    cv2.imshow(\"VGG16 - Top Vorhersage\", frame)\n",
    "\n",
    "    # Wenn die q Taste gedrückt wird, beenden wir die Endlosschleife\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e90e23-8187-4163-967e-e7a2afc4cdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16 Objekterkennung - Aufgabe\n",
    "import cv2\n",
    "import dlmodels\n",
    "\n",
    "# Start der Camera\n",
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "# Start unseres DL Objekterkennungs Modells\n",
    "model = dlmodels.VGG16ObjectDetector()  # VGG16 ist ein sehr einfaches, Neuronales Netzwerk für Objekterkennung\n",
    "\n",
    "while True:\n",
    "    error, frame = camera.read()\n",
    "    if not error:\n",
    "        break\n",
    "\n",
    "    # Aufgabe: Verwende das Modell um die Vorhersage (Label) zu berechnen\n",
    "\n",
    "    # Wir geben das Label links oben im Bild aus\n",
    "    cv2.putText(frame, labels[0], (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Und zeigen die Kamera an\n",
    "    cv2.imshow(\"VGG16 - Top Vorhersage\", frame)\n",
    "\n",
    "    # Wenn die q Taste gedrückt wird, beenden wir die Endlosschleife\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df49ac56-07b6-4df8-9a5d-cc226d64e340",
   "metadata": {},
   "source": [
    "Im oberem Beispiel geben wir nur die am besten passende Vorhersage aus. Das nennt man die Top-1 Prediction. Das ist oft nicht der beste Ansatz, da das Modell natürlich nicht perfekt ist. Manchmal ist es ein besserer Ansatz, die 5 am besten passenden Vorhersagen auszugeben. Hierfür könnt Ihr bei model.predict(frame, top5=True) angeben:\n",
    "```\n",
    "    labels = model.predict(frame, top5=True)\n",
    "```\n",
    "\n",
    "Da ihr jetzt nicht nur das beste Label bekommt sondern die 5 besten, müssen wir diese 5 Texte in unser Bild, der Reihe nach sortiert, einfügen.\n",
    "```\n",
    "    for i, label in enumerate(labels):\n",
    "        cv2.putText(frame, label, (10, 30 + i * 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "```\n",
    "\n",
    "Hierfür machen wir die Schrift etwas kleiner (0.6 anstelle von 1) und gehen jedes mal wenn wir Text einfügen, etwas weiter nach unten im Bild (10, 30 + i * 30).\n",
    "\n",
    "Gerne könnt ihr in dlmodel.py ausprobieren, was passiert wenn ihr anstelle top-5, top-10 ausgebt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2659db-eec7-4b58-9c97-5067d7196d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16 Top5 Objekterkennung - Aufgabe\n",
    "import cv2\n",
    "import dlmodels\n",
    "\n",
    "camera = cv2.VideoCapture(0)\n",
    "model = dlmodels.VGG16ObjectDetector()\n",
    "\n",
    "while True:\n",
    "    error, frame = camera.read()\n",
    "    if not error:\n",
    "        break\n",
    "\n",
    "    # Aufgabe: Verwende das Modell um die 5 besten Vorhersagen (Labels) zu berechnen\n",
    "\n",
    "    # Und geben alle 5, der Reihe nach sortiert, aus\n",
    "    for i, label in enumerate(labels):\n",
    "        cv2.putText(frame, label, (10, 30 + i * 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "    # Show camera feed\n",
    "    cv2.imshow(\"VGG16 - Top 5 Vorhersage\", frame)\n",
    "\n",
    "    # Exit when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a7f8d4-ea5c-40dd-86d3-c27bae439259",
   "metadata": {},
   "source": [
    "## Objekterkennung mit tiefen Neuronalen Netzwerken (ResNet-50)\n",
    "\n",
    "ResNet ist eine Verbesserung von VGG16, und ist in mehreren Stufen verfügbar. Hier verwenden wir ResNet-50. Um dieses zu verwenden kann \n",
    "```\n",
    "model = dlmodels.RestNet50ObjectDetector()\n",
    "```\n",
    "\n",
    "verwendet werden. Der Code wurde von mir so vorbereitet, das der Rest genau gleich wie beim VGG Beispiel verwendet werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070dfd1f-311e-49ea-a390-bd086937024a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet-50 Objekterkennung - Aufgabe\n",
    "import cv2\n",
    "import dlmodels\n",
    "\n",
    "camera = cv2.VideoCapture(0)\n",
    "# Anstelle von VGG verwenden wir hier das tieferen und für gewöhnlich besser funktionierende ResNet 50\n",
    "\n",
    "# Aufgabe: Lade das Modell\n",
    "\n",
    "while True:\n",
    "    # Aufgabe: Lade den Frame, sage das warscheindlichste Label voraus und gib Label sowie Frame aus\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42d92f8-a59a-42d7-9c69-74e8b73e2d21",
   "metadata": {},
   "source": [
    "Ich habe für ResNet-50 ebenfalls die Top-5 vorhersage programmiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205e0012-f986-43dd-bec5-31e180ee4e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet-50 Top-5 Objekterkennung - Aufgabe\n",
    "import cv2\n",
    "import dlmodels\n",
    "\n",
    "# Aufgabe: Anstelle Top-1, macht die ResNet-50 Top-5 Vorhersage\n",
    "\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a250bad8-a5cf-47e8-a7b1-93706d95c18c",
   "metadata": {},
   "source": [
    "ResNet-152 ist eine um einiges besserer Objekterkenner als ResNet-50. Der folgende Code verwendet \n",
    "```\n",
    "model = dlmodels.RestNet152ObjectDetector()\n",
    "```\n",
    "\n",
    "als Modell um Objekte zu erkennen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f0d06a-7190-4c09-96f8-1d452d1d3822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlmodels\n",
    "\n",
    "camera = cv2.VideoCapture(0)\n",
    "model = dlmodels.RestNet152ObjectDetector()\n",
    "\n",
    "while True:\n",
    "    error, frame = camera.read()\n",
    "    if not error:\n",
    "        break\n",
    "\n",
    "    labels = model.predict(frame, top5=True)  \n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        cv2.putText(frame, label, (10, 30 + i * 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"ResNet152 - Top 5 Vorhersage\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43822c20-148d-4af9-9a48-786257629d30",
   "metadata": {},
   "source": [
    "## MaskRCNN\n",
    "\n",
    "VGG und ResNet können zwar Objekte erkennen, aber nicht darstellen wo diese Objekte gefunden wurden. MaskRCNN kann einerseits Objekte erkennen und gibt andererseits an, wo diese Objekte gefunden wurden. Da MaskRCNN sehr viele Objekte gleichzeitig erkennt, ist dieses leider sehr langsam (das Bild wird in diesem Beispiel also sehr langsam sein).\n",
    "\n",
    "Ein wichtiger Teil dieses Beispiels ist der Schwellenwertparameter (threshold). Nur wenn sich das Modell sicherer ist wie dieser threshold, wird das Objekt auch als solches erkannt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f31c47c-3e66-40b4-afd8-6f5d45bbf3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import dlmodels\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "\n",
    "# Objekte welche über MaskRCNN erkannt werden können\n",
    "COCO_CLASSES = [\n",
    "    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "    'fire hydrant', 'street sign', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', \n",
    "    'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'hat', 'backpack', 'umbrella', 'shoe', 'eye glasses', \n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'plate', 'wine glass', 'cup', 'fork', 'knife', 'spoon',\n",
    "    'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut',\n",
    "    'cake', 'chair', 'couch', 'potted plant', 'bed', 'mirror', 'dining table', 'window', 'desk', 'toilet',\n",
    "    'door', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',\n",
    "    'sink', 'refrigerator', 'blender', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush',\n",
    "    'hairbrush'\n",
    "]\n",
    "\n",
    "# Der Wert (in %) welcher geprüft wird um ein Objekt zu erkennen. 0 sind 0%, 1 sind 100%, 0.5 stehen für 50%\n",
    "threshold = 0.8  # Das Modell muss sich mindestens zui 80% sicher sein um ein Objekt zu erkennen.\n",
    "\n",
    "# Aufgabe: Spielt euch ein bisschen mit dem Threshold. Was passiert wenn dieser näher 1 und näher 0 geht?\n",
    "\n",
    "# Laden des vortrainierten Mask R-CNN Modells\n",
    "model = dlmodels.MaskRCNN()  # Wir geben dem Modell die Objekte und den Schwellenwert\n",
    "\n",
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    error, frame = camera.read()\n",
    "    if not error:\n",
    "        break\n",
    "\n",
    "    gefundenePositionen, Labels, Warscheindlichkeit = model.predict(frame)\n",
    "\n",
    "    # Hier untersuchen wir jedes gefundene Objekt ...\n",
    "    for i in range(len(gefundenePositionen)):\n",
    "        # ... darauf ob die Warscheindlichkeit das es tatsächlich dieses Objekt ist ...\n",
    "        if Warscheindlichkeit[i] > threshold:\n",
    "            # ... und holen uns die Start und End Positionen des gefundenen Objektes ...\n",
    "            x1, y1, x2, y2 = map(int, gefundenePositionen[i])\n",
    "            label_id = Labels[i]\n",
    "\n",
    "            # ... zeichnen ein Rechteck rundherum und fügen einen Text darüber ein.\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"{label_id}_{COCO_CLASSES[label_id-1]}: {Warscheindlichkeit[i]:.2f}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Abschließend zeigen wir das Bild mit allen Rechtecken und Texten an.\n",
    "    cv2.imshow(\"Mask R-CNN\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb030c0-8bdf-4b86-a4d2-6c27cf9bdea0",
   "metadata": {},
   "source": [
    "## CLIP\n",
    "\n",
    "CLIP ist ein Modell das es uns ermöglicht, die Ähnlichkeit zwischen Bildern und Texten zu berechnen.\n",
    "Ein Bild welches einen Hund zeigt, ist für dieses Modell näher zu einem Text \"Das ist ein Hund\" als ein Text \"Pferd läuft über die Wiese\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc23e4d-a723-40b2-9849-5c1f5042728d",
   "metadata": {},
   "source": [
    "Für CLIP müssen wir ein paar weitere Modelle installieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8d8506-1ddf-4b34-8cfb-92fafea277af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ftfy regex tqdm\n",
    "%pip install git+https://github.com/openai/CLIP.git\n",
    "%pip install scikit-image\n",
    "%pip install pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221ccff1-5c83-42ff-918a-163b2ce3f57a",
   "metadata": {},
   "source": [
    "CLIP ist ein Modell, welches Bilder und Texte bearbeiten kann. Es ist darauf ausgelegt, die Ähnlichkeit von diesen Bildern und Texten heruaszufinden. Um CLIP zu verwenden könnt ihr:\n",
    "```\n",
    "model = dlmodels.ClipObjectAndTextEmbedding()\n",
    "```\n",
    "\n",
    "verwenden. CLIP verwendet ResNet-50 im Hintergrund zur Erkennung von Bildern und Transformer (die Technologie hinter ChatGPT) für die Texterkennung. Für dieses Beispiel verwenden wir Demo-Daten, welche wir über: \n",
    "```\n",
    "bilder, originalBilder = model.loadImages(namen)\n",
    "```\n",
    "\n",
    "geladen und über:\n",
    "```\n",
    "model.showImages(originalBilder, beschreibungen)\n",
    "```\n",
    "\n",
    "dargestellt werden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae10e24a-85cf-4364-aa27-b85e815eca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import dlmodels\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Wenn ihr eine GPU verbaut habt, könnt ihr hier device = \"cuda\" wählen, dann läuft alles viel schneller!\n",
    "device = \"cpu\"\n",
    "# Hier laden wir unser Clip Modell. Wenn es fertig geladen ist gibt es uns einige Sachen aus. \n",
    "# Wie groß sind die Bilder die dem Modell übergeben werden können, wie lange kann Text sein und wie viele Wörter kennt das Modell.\n",
    "model = dlmodels.ClipObjectAndTextEmbedding(device)\n",
    "\n",
    "# Wir laden jetzt 8 Bilder und vergleichen diese mittels Clip. Clip ermöglicht es uns, Texte und Bilder zu vergleichen, sprich zu schauen, wie gut Bilder und Texte zusammenpassen\n",
    "namen = [\"astronaut\",\"camera\",\"chelsea\",\"coffee\",\"horse\",\"motorcycle_right\",\"page\",\"rocket\"]\n",
    "beschreibungen = [\n",
    "    \"a portrait of an teacher with the American flag\",\n",
    "    \"a person looking at a camera on a tripod\",\n",
    "    \"a facial photo of a tabby cat\",\n",
    "    \"a cup of coffee on a saucer\",\n",
    "    \"a black-and-white silhouette of a horse\",\n",
    "    \"a red motorcycle standing in a garage\", \n",
    "    \"a page of text about segmentation\",\n",
    "    \"a rocket standing on a launchpad\"\n",
    "]\n",
    "\n",
    "# Laden der Bilder\n",
    "bilder, originalBilder = model.loadImages(namen)\n",
    "# Zeigen der Bilder\n",
    "model.showImages(originalBilder, beschreibungen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d3afa3-12cf-41f0-8cca-ef83e6b949e1",
   "metadata": {},
   "source": [
    "Um die Ähnlichkeit der Bilder und Texte zu berechnen kann\n",
    "```\n",
    "similarity = model.calculateSimilarity(bilder, beschreibungen)\n",
    "```\n",
    "\n",
    "verwendet werden. Mittels \n",
    "```\n",
    "model.plotSimilarity(originalBilder, beschreibungen, similarity)\n",
    "```\n",
    "\n",
    "werden diese Bilder, Texte und Ähnlichkeiten ausgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3610ebe2-842b-4980-a842-3e7a7d981d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Da Clip nicht nur Bilder sondern auch Text versteht können wirskimage.data_dir uns ansehen wie Clip unseren Text interpretiert.\n",
    "print(model.tokenize(\"Hallo! Wie geht es dir heute?\"))\n",
    "\n",
    "# Berechnen der Ähnlichkeit zwischen Bilder und Texte\n",
    "similarity = model.calculateSimilarity(bilder, beschreibungen)\n",
    "model.plotSimilarity(originalBilder, beschreibungen, similarity)\n",
    "\n",
    "# Mehr hier: https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb#scrollTo=NSSrLY185jSf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3026414f-e5bb-4584-b2bc-c1c5191e1a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlmodels\n",
    "\n",
    "camera = cv2.VideoCapture(0)\n",
    "# Wenn ihr eine GPU verbaut habt, könnt ihr hier device = \"cuda\" wählen, dann läuft alles viel schneller!\n",
    "device = \"cpu\"\n",
    "# Hier laden wir unser Clip Modell.\n",
    "model = dlmodels.ClipObjectAndTextEmbedding(device)\n",
    "\n",
    "# Aufgabe: Probiert unterschiedliche Texte aus!\n",
    "comparisonTexts = [\"A person looking straight at you\", \"a human holding a Fork\", \"A person standing with fruit in its hands\", \"an empty room\", \"a room with people working\"]\n",
    "\n",
    "while True:\n",
    "    error, frame = camera.read()\n",
    "    if not error:\n",
    "        break\n",
    "\n",
    "    frameForClip = model.processImage(frame)\n",
    "    similarity = model.calculateSimilarity(frameForClip, comparisonTexts)\n",
    "\n",
    "    # Sortieren der Ähnlichkeiten und Texte\n",
    "    sorted_texts = sorted(zip(comparisonTexts, similarity), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    best_match = sorted_texts[0][0]\n",
    "    cv2.putText(frame, best_match, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"CLIP - Ähnlichster Text\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba57c66-c0fb-475c-b939-94bba0c52d25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
