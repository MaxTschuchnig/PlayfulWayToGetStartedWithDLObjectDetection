{"cells":[{"cell_type":"markdown","id":"2732019c-1c22-4f83-9946-cf3154ebf8ce","metadata":{"id":"2732019c-1c22-4f83-9946-cf3154ebf8ce"},"source":["# SFZ Kurs - KI trifft Python: Dein Weg zur eigenen Objekterkennung\n","\n","## Inhalt der Einheit: Einführung in die Objekterkennung\n","- Einführung in Python\n","- Einlesen von Kameras\n","- Bearbeiten von Kamerabildern - Wie sieht ein PC\n","- Einfache Objekterkennung\n","- Gesichtserkennung\n","\n","![Face Detection](./Facedetection.png)\n"]},{"cell_type":"markdown","id":"2d1361ab-be56-44d9-ba85-90ea3596eb50","metadata":{"id":"2d1361ab-be56-44d9-ba85-90ea3596eb50"},"source":["## Einführung in Python\n","\n","Python ist eine interpretierte Programmiersprache welche sehr viele Module anbietet.\n","Wir werden diese Module verwenden um eine einfache Objekterkennung von Grund auf zu erstellen.\n","\n","Hierfür müssen wir jedoch erst ein paar grundlegende Python Grundlagen verstehen:\n","\n","### Variablen\n","Variablen ermöglichen es euch, Werte zu speichern und später wiederzuverwenden. Z.B.:\n","```\n","wert1 = 5\n","wert2 = wert1 + 7\n","```\n","Nun ist a wert1 gleich 5 und wert2 gleich 12 gesetzt.\n","\n","### Abfragen\n","Um Abfragen zu ermöglichen (Wenn/Dann) kann in python if verwendet werden. Um die nicht zutreffenden Fälle abzubilden wird else verwendet. Z.B.:\n","```\n","if wert1 < 10:\n","    print(\"Wert ist kleiner 10\")\n","elif wert1 == 100:\n","    print(\"Wert ist genau 100\")\n","else:\n","    print(\"Wert is größer oder gleich 10\")\n","```\n","\n","### Schleifen\n","Mit Schleifen kann Code ausgeführt werden, bis eine bestimmte Bedingung zutrifft. Wir können zum Beispiel so lange zählen bis wir bei der Zahl 20 angekommen sind. Hierfür kann in Python while oder for verwendet werden. Z.B.:\n","```\n","for counter in range(0,20):\n","    print(counter)\n","```\n","oder\n","```\n","counter = 0\n","while counter < 20:\n","    print(counter)\n","    counter += 1\n","```\n"]},{"cell_type":"code","execution_count":null,"id":"a0322548-7ede-4e52-ae75-e743c0c305e4","metadata":{"id":"a0322548-7ede-4e52-ae75-e743c0c305e4"},"outputs":[],"source":["# Variablen Code\n","wert1 = 5\n","wert2 = \"Hallo Welt\"\n","print(wert1)  # Gibt 5 aus\n","print(wert2)  # Gibt Hallo Welt aus"]},{"cell_type":"code","execution_count":null,"id":"0e48186e-03b3-4d2a-88d0-2990d7aa2500","metadata":{"id":"0e48186e-03b3-4d2a-88d0-2990d7aa2500"},"outputs":[],"source":["# Abfrage (Wenn/Dann) Code\n","if wert1 < 10:\n","    print(\"Wert ist kleiner 10\")\n","else:\n","    print(\"Wert is größer oder gleich 10\")"]},{"cell_type":"code","execution_count":null,"id":"182f13a2-f043-43fe-9328-1f719c32f9d7","metadata":{"id":"182f13a2-f043-43fe-9328-1f719c32f9d7"},"outputs":[],"source":["for counter in range(0,5):\n","    print(counter)\n","\n","counter = 0\n","while counter > -5:\n","    print(counter)\n","    counter = counter - 1"]},{"cell_type":"markdown","id":"c4124664-599c-4002-b338-cd3468f65583","metadata":{"id":"c4124664-599c-4002-b338-cd3468f65583"},"source":["### Funktionen\n","Oft ist es beim Programmieren notwendig, den selben Code an mehreren Stellen anzuwenden. Damit hier der Code nicht mehrmals geschrieben werden muss, können so genannte Funktionen verwendet werden. Z.B. müssen wir mehrere Bilder im Internet laden. Der Code hierfür ist relativ lange, daher definieren wir eine Funktion die für uns das Bild lädt:\n","\n","```\n","def load_image_cv2(url):\n","    # ... hier passiert viel code\n","    img = # ... hier wird das Bild geladen\n","    return img  # Hier wird das geladene Bild zurückgegeben\n","```\n","\n","In unserem Code können wir dann über diese Funktion das Bild laden\n","```\n","geladenesBild = load_image('https://images.pexels.com/photos/1563356/pexels-photo-1563356.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1')\n","```"]},{"cell_type":"markdown","id":"d3ffe4a1-3e5c-4c7e-ab8a-d805d917e720","metadata":{"id":"d3ffe4a1-3e5c-4c7e-ab8a-d805d917e720"},"source":["## Installation von Modulen\n","In Python können bestimmte Funktionen, wie das Laden einer Kamera oder das Training eines Neuronalen Netzen, über so genannte Module geladen werden. Ihr müsst also nicht alles neu programmieren. Der nächste Block installiert die in diesem Tutorial verwendeten Module mittels\n","\n","```\n","pip install\n","```\n","\n","Danach können diese Module über import, z.B.:\n","\n","```\n","import cv2\n","```\n","\n","verwendet werden"]},{"cell_type":"code","execution_count":null,"id":"c27d01a0-4a74-4cc0-9c44-817b53edd1bb","metadata":{"id":"c27d01a0-4a74-4cc0-9c44-817b53edd1bb"},"outputs":[],"source":["%pip install opencv-python matplotlib numpy"]},{"cell_type":"markdown","id":"9f4f8805-f5a0-4b88-98c5-ced8f6eef9f2","metadata":{"id":"9f4f8805-f5a0-4b88-98c5-ced8f6eef9f2"},"source":["- opencv lässt uns auf die Kamera zugreifen und coole Dinge damit machen\n","- matplotlib verwenden wir um Bilder darzustellen\n","- numpy wird verwendet um die Bearbeitung von Bildern zu vereinfachen"]},{"cell_type":"markdown","id":"d0b9670d-19dc-45ce-b94b-99668e2e4cf3","metadata":{"id":"d0b9670d-19dc-45ce-b94b-99668e2e4cf3"},"source":["## Verwenden der Kamera\n","\n","Um eine Kamera einzubinden können wir das Modul cv2 verwenden. Mithilfe von matplotlib können wir Bilder darstellen.\n","\n","```\n","import cv2\n","import matplotlib.pyplot as plt\n","```\n","\n","Über cv2.VideoCapture(0) können wir daraufhin auf die stardard Kamera zugreifen (Hinweis VideoCapture ist eine Funktion, an welche wir das Argument 0 übergeben, welches die erste Kamera ist welche das System kennt, also die standard Kamera).\n","\n","```\n","camera = cv2.VideoCapture(0)\n","```\n","\n","Über camera.read() können wir dann die Kamera \"lesen\" was euch den aktuellen Frame (das aktuell aufgenommen Bild) wiedergibt. Außerdem wird ein Fehler zurückgegeben wenn etwas nicht funktioniert.\n","\n","```\n","error, frame = camera.read()\n","```\n","\n","Um nun diesen Frame (das Bild) darzustellen können wir plt.imshow(frame) verwenden. Um das Bild danach tatsächlich anzuzeigen müssen wir noch plt.show() verwenden.\n","\n","```\n","plt.imshow(frame)\n","plt.show()\n","```\n","\n","Wichtig am Ende: Wir müssen die Kamera wieder freigeben!\n","\n","```\n","cap.release()\n","```"]},{"cell_type":"code","execution_count":null,"id":"4640afcd-4e4b-490b-a460-4e65efdd95d4","metadata":{"id":"4640afcd-4e4b-490b-a460-4e65efdd95d4"},"outputs":[],"source":["import cv2\n","import matplotlib.pyplot as plt\n","\n","# Aufgabe: Greife richtig auf die Kamera zu und zeige den aktuellen Frame an.\n","#          Fällt dir hierbei etwas spezielles auf?\n","\n","camera.release()"]},{"cell_type":"markdown","id":"837ba275-a5fa-46dd-95fd-93c635284b2f","metadata":{"id":"837ba275-a5fa-46dd-95fd-93c635284b2f"},"source":["### Vertauschte Farben?\n","\n","Wie ihr in dem Beispiel sehen könnt liefert cv2 die Bilder in BGR (Blau-Grün-Rot Format), matplotlib nimmt diese aber als RGB an, daher ist rot und blau vertauscht! Über cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) können wir den Farbbereich von BGR in RGB umwandeln\n","\n","```\n","frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","```"]},{"cell_type":"code","execution_count":null,"id":"be0d51a3-cadc-440f-b585-bb8efb45a74a","metadata":{"id":"be0d51a3-cadc-440f-b585-bb8efb45a74a"},"outputs":[],"source":["# Jetzt sind die Farben richtig :)\n","import cv2\n","import matplotlib.pyplot as plt\n","\n","camera = cv2.VideoCapture(0)\n","error, frame = camera.read()\n","frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","\n","plt.imshow(frame)\n","plt.show()\n","\n","camera.release()"]},{"cell_type":"markdown","id":"70439c36-d415-49bc-9d88-03e4ec5134e5","metadata":{"id":"70439c36-d415-49bc-9d88-03e4ec5134e5"},"source":["## Live Video\n","\n","Bis jetzt nehmen wir immer nur ein Bild auf, etwas langweilig. Wir können aber in einer Endlosschleife immer einen neuen Frame aufnehmen und diesen ausgeben. Eine Endlosschleife kann mit while 1==1: definiert werden (bedeutet: führe das so lange aus wie 1==1 ist, was es immer sein wird, daher Endlosschleife). Alternativ kann auch einfach while True verwendet werden\n","\n","```\n","while True:\n","    print('abc')  # Gibt, bis es unterbrochen wird, 'abc' aus. Ich empfehle NICHT diesen Code auszuführen\n","```\n","\n","Wenn wir nun in diese Endlosschleife das gleiche wie vorher machen, haben wir eine Live Kamera! Wir brauchen jedoch noch einen Weg um aus der Endlosschleife auszubrechen, sobald wir fertig sind. Mittels diesem Code können wir in der Schleife checken, ob in einer cv2 Anwendung eine Taste gedrückt wurde: if cv2.waitKey(1) & 0xFF == ord('q'). An diesem Code ist hauptsächlich ord('q') wichtig. Hiermit gebt Ihr an auf welche Taste gehört wird (In diesem Beispiel achten wir darauf ob die Q Taste gedrückt wurde). Mithilfe von break, kann die Endlosschleife unterbrochen werden.\n","\n","```\n","if cv2.waitKey(1) & 0xFF == ord('q'):  # Bricht die Schleife ab, wenn 'q' gedrückt wird\n","    break\n","```\n","\n","Da wir nur in cv2 Anwendungen auf unsere Tastendrucke höhren können, müssen wir die Frames über cv2 Ausgeben, nicht über matplotlib. Anstelle von plt.imshow(frame) und plt.show() verwenden wir nun also cv2.imshow(\"Camera\", frame)\n","\n","```\n","cv2.imshow(\"Camera\", frame)\n","```\n"]},{"cell_type":"code","execution_count":null,"id":"a6c18cdb-137b-4985-bca5-cc567fbe790c","metadata":{"id":"a6c18cdb-137b-4985-bca5-cc567fbe790c"},"outputs":[],"source":["# Live Video Beispiel\n","import cv2\n","\n","camera = cv2.VideoCapture(0)\n","\n","while True:  # Endlosschleife\n","\n","    # Aufgabe: Greife richtig auf die Kamera zu und zeige den aktuellen Frame an.\n","\n","    if cv2.waitKey(1) & 0xFF == ord('q'):  # Bricht die Schleife ab, wenn 'q' gedrückt wird\n","        break\n","\n","camera.release()\n","cv2.destroyAllWindows()  # Schließt das Fenster nach Beenden der Schleife"]},{"cell_type":"markdown","id":"d5751f7b-8772-42f4-b893-7b2907cbac2b","metadata":{"id":"d5751f7b-8772-42f4-b893-7b2907cbac2b"},"source":["## Live Video Bearbeitung\n","\n","Da wir den Frame nun in unserem Program haben, können wir diesen natürlich auch live verändern.\n","\n","### Helligkeit\n","\n","Eine einfache Veränderung ist die Helligkeit des Frames. Ein Frame ist eigentlich nur eine Anordnung vieler, 3 farbiger Pixel mit Werten von 0-255. Wenn wir nun die Helligkeit erhöhen oder verringern wollen müssen wir einfach jeden einzelnen Pixel um einen bestimmten Anteil erhöhen/verringern. Das kann in cv2 mit der Funktion cv2.convertScaleAbs gemacht werden.\n","\n","```\n","frame = cv2.convertScaleAbs(frame, alpha=newbrightness, beta=0)  # Wenn newbrightness kleiner als 1 ist wird die Helligkeit reduziert, bei größer 1 wird diese erhöht\n","```\n","\n","Z.B. hat ein blauer Punkt die RGB (Rot-Grün-Blau) Werte: (0, 0, 255). Wenn wir diesen Punkt jetzt etwas dunkler machen wollen können wir den Punkt mit 0.9 multiplizieren. Danach sind die Werte des Punktes (0, 0, 229)!"]},{"cell_type":"code","execution_count":null,"id":"9bb2f447-d670-40af-9f33-e80aabaf740f","metadata":{"id":"9bb2f447-d670-40af-9f33-e80aabaf740f"},"outputs":[],"source":["# Live Video Bearbeitung - Helligkeit\n","import cv2\n","cap = cv2.VideoCapture(0)\n","\n","brightness = 1.0  # Speichert die Helligkeitsänderung des Bildes\n","\n","while True:\n","    error, frame = cap.read()\n","    if not error:\n","        break\n","\n","    frame = cv2.convertScaleAbs(frame, alpha=brightness, beta=0)  # Hier übergeben wir einen Faktor alpha, welcher die Helligkeit des Frames steuert (multipliziert)\n","\n","    cv2.imshow(\"Camera\", frame)\n","    key = cv2.waitKey(1) & 0xFF  # Liest gedrückte Tasten\n","\n","    if key == ord('q'):  # Wenn q --> abbrechen\n","        break\n","    elif key == ord('+'):  # Wenn + --> Helligkeit erhöhen\n","        # Aufgabe, wenn + gedrückt wird soll die Helligkeit um 0.1 erhöht werden\n","    elif key == ord('-'):  # Wenn - --> Helligkeit veringern\n","        # Aufgabe, wenn - gedrückt wird soll die Helligkeit um 0.1 verringert werden\n","\n","cap.release()\n","cv2.destroyAllWindows()\n"]},{"cell_type":"markdown","id":"79905bf8-f349-4288-84d8-004185330664","metadata":{"id":"79905bf8-f349-4288-84d8-004185330664"},"source":["### Schwarz-Weiß Bild\n","\n","Mithilfe der Funktion cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) können wir unser Farbbild in ein Schwarz-Weiß Bild umwandeln.\n","\n","```\n","frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","```\n","\n","Hinweis: Der Wert gray im Beispiel ist ein so genannter Wahrheitswert (Boolean). Dieser kann entweder True oder False sein. Um eine Wahrheitswert von True auf False zu ändern kann\n","```\n","gray = not gray\n","```\n","verwendet werden.\n"]},{"cell_type":"code","execution_count":null,"id":"9d6b906d-22e8-41a8-ac9c-d442dbebe8aa","metadata":{"id":"9d6b906d-22e8-41a8-ac9c-d442dbebe8aa"},"outputs":[],"source":["# Live Video Bearbeitung - Schwarz-Weiß Bild\n","import cv2\n","cap = cv2.VideoCapture(0)\n","\n","brightness = 1.0\n","gray = False  # Hier speichern wir ab ob wir aktuell ein Schwarz-Weiß Bild darstellen oder nicht (True -> Ja, False -> Nein)\n","\n","while True:\n","    error, frame = cap.read()\n","    if not error:\n","        break\n","\n","    if gray:  # Wenn wir ein Schwarz-Weiß Bild darstellen wollen\n","        # Aufgabe: Überschreibe frame mit dem Schwarz-Weiß Bild\n","    frame = cv2.convertScaleAbs(frame, alpha=brightness, beta=0)\n","\n","    cv2.imshow(\"Camera\", frame)\n","    key = cv2.waitKey(1) & 0xFF  # Liest gedrückte Tasten\n","\n","    if key == ord('q'):\n","        break\n","    elif key == ord('+'):\n","        brightness += 0.1\n","    elif key == ord('-'):\n","        brightness = max(0.1, brightness - 0.1)\n","    elif key == ord('g'):  # Wenn g --> Wechsel auf Schwarz-Weiß Bild\n","        # Aufgabe: Wenn die Taste g gedrückt wird soll sich gray von True auf False oder umgekehrt ändern, sprich,\n","        #wenn wir ein Farbbild darstellen sollen wir danach ein Schwarz-Weiß Bild darstellen und umgekehrt\n","\n","cap.release()\n","cv2.destroyAllWindows()\n"]},{"cell_type":"markdown","id":"eea59156-b0dc-4094-85ed-a9aadb9327d1","metadata":{"id":"eea59156-b0dc-4094-85ed-a9aadb9327d1"},"source":["## Was kann der PC eigentlich sehen?\n","\n","Um diese Frage zu beantworten können wir uns den folgenden Code ausführen:"]},{"cell_type":"code","execution_count":null,"id":"b5c57c8b-d558-4b73-9ee5-d4ed674ca9b5","metadata":{"id":"b5c57c8b-d558-4b73-9ee5-d4ed674ca9b5"},"outputs":[],"source":["import cv2\n","\n","cap = cv2.VideoCapture(0)\n","\n","brightness = 1.0\n","gray = False\n","\n","def get_pixel_value(event, x, y, flags, param):\n","    if event == cv2.EVENT_LBUTTONDOWN:  # Left mouse button click\n","        if gray:\n","            print(f\"Clicked at ({x}, {y}): Value={frame[y, x]}\")\n","        elif not gray:\n","            b, g, r = frame[y, x]  # Get BGR values\n","            print(f\"Clicked at ({x}, {y}): R={r}, G={g}, B={b}\")\n","\n","# Create OpenCV window and set mouse callback\n","cv2.namedWindow(\"Camera\")\n","cv2.setMouseCallback(\"Camera\", get_pixel_value)\n","\n","ret, frame = cap.read()\n","\n","while True:\n","    cv2.imshow(\"Camera\", frame)\n","\n","    key = cv2.waitKey(1) & 0xFF  # Liest gedrückte Tasten\n","\n","    if key == ord('q'):\n","        break\n","    elif key == ord('+'):\n","        frame = cv2.convertScaleAbs(frame, alpha=1.1, beta=0)\n","    elif key == ord('-'):\n","        frame = cv2.convertScaleAbs(frame, alpha=0.9, beta=0)\n","    elif key == ord('g'):  # Wenn g --> Wechsel auf Graubild\n","        gray = not gray\n","        if gray:\n","            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","        elif not gray:\n","            ret, frame = cap.read()\n","\n","cap.release()\n","cv2.destroyAllWindows()\n"]},{"cell_type":"markdown","id":"2b814bfc-19e4-4c58-a0a8-97c685975079","metadata":{"id":"2b814bfc-19e4-4c58-a0a8-97c685975079"},"source":["In diesem Code passiert sehr viel änlich wie vorher:\n","- Module werden geladen\n","- Die Kamera wird geladen\n","- Wir haben eine Endlosschleife\n","- Unser Frame wird geladen\n","- Helligkeit und Schwarz-Weiß -> Farbbild ist ermöglicht\n","- An Ende wird alles sauber zusammengeräumt\n","\n","Zusätzlich wird die Kamera ohne Frame (also als leeres Bild) geöffnet und vermerkt was gemacht werden soll, wenn auf dieses Fenster geklickt wird (bestimmen wir in der get_pixel_value Funktion).\n","\n","```\n","cv2.namedWindow(\"Camera\")\n","cv2.setMouseCallback(\"Camera\", get_pixel_value)\n","```\n","\n","Bei einem Linken Mausklick in das Fenster wird als erstes überprüft ob das Bild ein Farbbild ist oder nicht und dann werden entweder die Rot-Grün-Blau Anteile oder der Grauwert ausgegeben.\n","\n","```\n","def get_pixel_value(event, x, y, flags, param):\n","    if event == cv2.EVENT_LBUTTONDOWN:  # Left mouse button click\n","        if gray:\n","            print(f\"Clicked at ({x}, {y}): Value={frame[y, x]}\")\n","        elif not gray:\n","            b, g, r = frame[y, x]  # Get BGR values\n","            print(f\"Clicked at ({x}, {y}): R={r}, G={g}, B={b}\")\n","```\n","\n","**Diese Werte die hier ausgegeben werden sind das, was euer PC sehen kann!**"]},{"cell_type":"markdown","id":"59ca5166-8095-473a-b62e-9909bb526ea8","metadata":{"id":"59ca5166-8095-473a-b62e-9909bb526ea8"},"source":["### Formen in Live Videos\n","\n","Objekterkennung beschreibt zwei unterschiedliche Aufgaben:\n","- Erkennen welche Objekte im Frame vorkommen\n","- Erkennen wo diese Objekte sind\n","\n","Hierfür ist es praktisch Formen (Rechtecke) in einen Frame einzuzeichnen um das erkannte Objekt hervorzuheben. In cv2 kann über cv2.rectangle() ein Rechteck in einen Frame gezeichnet werden.\n","\n","```\n","cv2.rectangle(frame, (10, 10), (10+150, 10+150), (0, 255, 0), 2)\n","```\n","\n","Der gegebene Code zeichnet ein Rechteck in den Frame, welches auf Position (10,10) beginnt und bis Position (160,160) reicht. Es ist also ein 150 Pixel großes Quadrat. Die Farbe ist (0, 255, 0) was in Rot-Grün-Blau ein grünes Rechteck ergibt. Die dicke der Linie ist auf 2 gesetzt.\n"]},{"cell_type":"code","execution_count":null,"id":"c6ba7946-881a-4b77-a252-796eb94f564f","metadata":{"id":"c6ba7946-881a-4b77-a252-796eb94f564f"},"outputs":[],"source":["# Live Video Bearbeitung - Formen einfügen\n","import cv2\n","\n","cap = cv2.VideoCapture(0)\n","\n","rectangle = False  # Soll aktuell ein recteck gezeichnet werden?\n","rectangle_pos = (0,0)\n","\n","def get_rectangle_position(event, x, y, flags, param):  # Diese Funktion wird ausgeführt sobald auf den Frame geklickt wird. x und y geben an welcher Pixel geklickt wurde\n","    global rectangle, rectangle_pos\n","    if event == cv2.EVENT_LBUTTONDOWN:  # Linke Maustaste wurde gecklickt\n","        rectangle = True\n","        rectangle_pos = # Aufgabe: wie können wir den Startpunkt des Rechtecks richtig setzen?\n","\n","cv2.namedWindow(\"Camera\")\n","cv2.setMouseCallback(\"Camera\", get_rectangle_position)\n","\n","while True:\n","    error, frame = cap.read()\n","    if not error:\n","        break\n","\n","    if rectangle:\n","        # Aufgabe: Hier soll ein Rechteck in den frame gezeichnet werden\n","        # mit (rectangle_pos[0], rectangle_pos[1]) als Startpunkt\n","        # (rectangle_pos[0]+150, rectangle_pos[1]+150) als Endpunkt\n","        # grüner Farbe\n","        # und einer Liniendicke von 2\n","\n","    cv2.imshow(\"Camera\", frame)\n","\n","    if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to quit\n","        break\n","\n","cap.release()\n","cv2.destroyAllWindows()\n"]},{"cell_type":"markdown","id":"36352de3-930a-423f-843d-81986858ecd7","metadata":{"id":"36352de3-930a-423f-843d-81986858ecd7"},"source":["### Text in Live Video\n","\n","Oft ist es auch sehr praktisch, Text in einen Frame einfügen zu können, z.B. um zu beschreiben was gefunden wurde. Über cv2 kann das mittels cv2.putText() gemacht werden. Ähnlich zum Rechteck hat diese Funktion einige Parameter wie\n","\n","- text = \"WAS WURDE GEFUNDEN\"\n","- font = cv2.FONT_HERSHEY_SIMPLEX  # eine Schriftart, mehr siehe hier: https://codeyarns.com/tech/2015-03-11-fonts-in-opencv.html#gsc.tab=0\n","- font_scale = 1  # Scalierung (Vergrößerung und Verkleinerung der Schrift)\n","- color = (0, 255, 0)  # Farbe\n","- thickness = 1  # Dicke\n","\n","Dann kann der Text mittels\n","\n","```\n","cv2.putText(frame, text, (rectangle_pos[0], rectangle_pos[1]), font, font_scale, color, thickness)\n","```\n","\n","eingefügt werden"]},{"cell_type":"code","execution_count":null,"id":"51ad4c59-47a9-4276-a8ee-dcd5b7b72639","metadata":{"id":"51ad4c59-47a9-4276-a8ee-dcd5b7b72639"},"outputs":[],"source":["# Live Video Bearbeitung - Text einfügen\n","import cv2\n","\n","cap = cv2.VideoCapture(0)\n","\n","rectangle = False\n","rectangle_pos = (0,0)\n","\n","def get_rectangle_position(event, x, y, flags, param):\n","    global rectangle, rectangle_pos\n","    if event == cv2.EVENT_LBUTTONDOWN:\n","        rectangle = True\n","        rectangle_pos = (x,y)\n","\n","cv2.namedWindow(\"Camera\")\n","cv2.setMouseCallback(\"Camera\", get_rectangle_position)\n","\n","while True:\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","\n","    if rectangle:\n","        cv2.rectangle(frame, (rectangle_pos[0], rectangle_pos[1]), (rectangle_pos[0]+100, rectangle_pos[1]+100), (0, 255, 0), 2)\n","\n","        # Aufgabe: Hier soll euer Name über dem Rechteck dargestellt werden\n","\n","    cv2.imshow(\"Camera\", frame)\n","\n","    if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to quit\n","        break\n","\n","cap.release()\n","cv2.destroyAllWindows()\n"]},{"cell_type":"markdown","id":"0c0c00c3-789c-4823-b70e-5d3b04e1fe7f","metadata":{"id":"0c0c00c3-789c-4823-b70e-5d3b04e1fe7f"},"source":["## Einfache Objekterkennung\n","\n","Bevor wir mit komplexer Objekterkennung mittels tiefem Lernen starten, werden wir uns konventionelle Objekterkennung ansehen. Die einfachste Objekterkennung basiert auf der globalen Analyse von Farben in Bildern. Mittels Quantisierung, welche im nächstem Codeabschnitt gezeigt wird, kann diese Farbanalyse verbessert werden.\n","\n","Ihr könnt versuchen den folgenden Objekterkennungscode mit Quantisierung zu verbessern"]},{"cell_type":"code","execution_count":null,"id":"2a325865-4075-48f7-aa01-d53cfa426698","metadata":{"id":"2a325865-4075-48f7-aa01-d53cfa426698"},"outputs":[],"source":["# Quantisierung Beispiel\n","import cv2\n","import numpy as np\n","\n","def simple_quantization(image, levels=4):\n","    \"\"\" Reduce the number of colors using bit-shifting \"\"\"\n","    if levels < 2:\n","        levels = 2\n","    shift = 256 // levels  # Compute step size\n","    quantized = (image // shift) * shift  # Reduce color depth\n","    return quantized\n","\n","cap = cv2.VideoCapture(0)\n","\n","while True:\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","\n","    quantized_frame = simple_quantization(frame, levels=4)  # Über den levels Parameter kann die Quantisierung eingestellt werden.\n","    # Wenn dieser Parameter sehr niedrig ist, werden ähnliche Farben in die gleiche Farbe übersetzt, sprich, es gibt weniger Farbunterschiede im Bild\n","\n","    cv2.imshow(\"Simple Quantization\", quantized_frame)\n","\n","    if cv2.waitKey(1) & 0xFF == ord('q'):\n","        break\n","\n","cap.release()\n","cv2.destroyAllWindows()\n"]},{"cell_type":"markdown","id":"810d27e3-04f2-4093-afde-a7117f28caa5","metadata":{"id":"810d27e3-04f2-4093-afde-a7117f28caa5"},"source":["### Farbenbasierte Objekterkennung in Bildern\n","\n","In den folgenden Beispielen laden wir Bilder aus dem Internet und verwenden Farben um Objekte vom Hintergrund zu trennen (ähnlich wie ein Green-Screen). Um ein Bild aus dem Internet zu laden könnt Ihr die load_image_cv2(URL) Funktion verwenden. Wenn alles gut läuft, könnt ihr das Bild so laden:\n","\n","```\n","blueCar = \"https://media.istockphoto.com/id/2158624488/photo/skoda-enyaq-electric-car.webp?s=2048x2048&w=is&k=20&c=UpH5XPNids_ZIPH-nfGxPTPe8N1m4_facFMFkkhX_4s=\"\n","blue_car_img = load_image_cv2(blueCar)\n","```"]},{"cell_type":"code","execution_count":null,"id":"545ebe80-68b6-4f1b-989c-e1688d91315f","metadata":{"id":"545ebe80-68b6-4f1b-989c-e1688d91315f"},"outputs":[],"source":["def load_image_cv2(url):\n","    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n","    response = requests.get(url, stream=True, headers=headers)\n","    if response.status_code == 200:\n","        img_array = np.asarray(bytearray(response.content), dtype=np.uint8)\n","        img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB for correct colors\n","        return img\n","    else:\n","        print(\"Failed to load image:\", url)\n","        return None"]},{"cell_type":"markdown","id":"44c500ac-3ca9-443c-a654-132d5a8b380f","metadata":{"id":"44c500ac-3ca9-443c-a654-132d5a8b380f"},"source":["**Die Idee ist nun diese:**\n","Nach dem erfolgreichem Laden eines Bildes können wir Objekte in diesem finden indem wir uns die Farben im Bild genauer ansehen. In diesem Beispiel ist das Auto sehr Blau, wärend der Hintergrund grün und grau ist. Wenn wir also in unserem Bild die Blaue Farbe analysieren (in RGB ist das die letzte Farbe) können wir sagen wir haben das Auto gefunden, wenn das Bild sehr Blau ist.\n","\n","![Blaues Auto](./blueCar.png)\n","\n","Um die unterschiedlichen Farben des Bildes zu analysieren können wir\n","- rotbild = bild[:, :, 0]  # Rote\n","- grünbild = bild[:, :, 1]  # Grüne\n","- blaubild = bild[:, :, 2]  # Blau\n","\n","verwenden. Danach können wir überprüfen ob das Bild hohe Blauwerte aufweist und gleichzeitig wenige Grün- und Rotwerte.\n","- blue_channel > 100\n","- green_channel < 100\n","- red_channel < 100\n","\n"]},{"cell_type":"code","execution_count":null,"id":"f7ab2319-9e87-4d09-b70d-bcae870de015","metadata":{"id":"f7ab2319-9e87-4d09-b70d-bcae870de015"},"outputs":[],"source":["import requests\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Addresse zu den Bildern (sehr gerne könnt ihr eure eigenen Bilder verwenden):\n","blueCar = \"https://media.istockphoto.com/id/2158624488/photo/skoda-enyaq-electric-car.webp?s=2048x2048&w=is&k=20&c=UpH5XPNids_ZIPH-nfGxPTPe8N1m4_facFMFkkhX_4s=\"\n","\n","# Hier laden wir die Bilder\n","blue_car_img = load_image_cv2(blueCar)\n","\n","# Code um das Blaue Auto zu finden\n","red_channel, green_channel, blue_channel = blue_car_img[:, :, 0], blue_car_img[:, :, 1], blue_car_img[:, :, 2]\n","blue_threshold, green_threshold, red_threshold = blue_channel > 100, green_channel < 100, red_channel < 100\n","\n","print('Je heller dieses Bild, umso höher ist der Blauwert im Bild')\n","plt.imshow(blue_channel, cmap='gray')\n","plt.show()\n","print('Alles was weiß ist hat einen höheren Blauwert wie 100. Warum ist dann der Boden weiß?')\n","plt.imshow(blue_threshold, cmap='gray')\n","plt.show()\n","print('Und warum ist das klar blaue Auto grün???')\n","plt.imshow(green_threshold, cmap='gray')\n","plt.show()"]},{"cell_type":"markdown","id":"8a9b5c1b-ab9d-45fb-930c-4227928902a7","metadata":{"id":"8a9b5c1b-ab9d-45fb-930c-4227928902a7"},"source":["**Je heller das Bild, umso mehr der Grün- und Rotkanäle ist im Bild enthalten.** Um also das Auto korrekt zu finden **muss**\n","- Das Bild einen besonders hohen Blauwert aufweisen und gleichzeitig\n","- einen geringeren Grün- und Rotwert\n","- Abschließend müssen wir alle 3 Ausschliesverfahren kombinieren um das Auto zu finden"]},{"cell_type":"code","execution_count":null,"id":"dc43e6e1-9a58-47c0-ab29-a17014baf612","metadata":{"id":"dc43e6e1-9a58-47c0-ab29-a17014baf612"},"outputs":[],"source":["# In diesem Beispiel wird diese Art der Objekterkennung dargestellt\n","import requests\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Addresse zu den Bildern (sehr gerne könnt ihr eure eigenen Bilder verwenden):\n","blueCar = \"https://media.istockphoto.com/id/2158624488/photo/skoda-enyaq-electric-car.webp?s=2048x2048&w=is&k=20&c=UpH5XPNids_ZIPH-nfGxPTPe8N1m4_facFMFkkhX_4s=\"\n","orangeCar = \"https://media.istockphoto.com/id/1485360485/photo/%C5%A1koda-octavia.webp?s=2048x2048&w=is&k=20&c=q5mE880v_twxD1ghU58k13PF_Bf3xhILgSSKUNNvUfQ=\"\n","\n","# Hier laden wir die Bilder\n","blue_car_img = load_image_cv2(blueCar)\n","orange_car_img = load_image_cv2(orangeCar)\n","\n","# Auswahl der Farbkanäle rot, grün und blau\n","red_channel, green_channel, blue_channel = blue_car_img[:, :, 0], blue_car_img[:, :, 1], blue_car_img[:, :, 2]\n","# Setzen der Erkenner für rot, grün und blau\n","blue_threshold = blue_channel > 100\n","green_threshold = green_channel < 100\n","red_threshold = red_channel < 100\n","# Kombinieren des Ausschliesverfahren\n","blue_mask = blue_threshold & green_threshold & red_threshold\n","\n","\n","plt.subplot(1, 2, 1)\n","plt.imshow(blue_car_img)\n","# plt.axis(\"off\")  # Wenn euch die Axen stören könnt ihr diese so ausstellen\n","plt.subplot(1, 2, 2)\n","plt.imshow(blue_mask, cmap='gray')\n","# plt.axis(\"off\")  # Wenn euch die Axen stören könnt ihr diese so ausstellen\n","plt.show()\n","\n","\n","# Gleiches für das orange Auto\n","red_channel, green_channel, blue_channel = orange_car_img[:, :, 0], orange_car_img[:, :, 1], orange_car_img[:, :, 2]\n","blue_threshold = blue_channel < 25\n","green_threshold = green_channel < 100\n","red_threshold = red_channel > 100\n","\n","orange_mask = blue_threshold & green_threshold & red_threshold\n","\n","plt.subplot(1, 2, 1)\n","plt.imshow(orange_car_img)\n","plt.subplot(1, 2, 2)\n","plt.imshow(orange_mask, cmap='gray')\n","plt.show()\n"]},{"cell_type":"markdown","id":"b6beaade-f16c-49a0-9916-73b7a2316e61","metadata":{"id":"b6beaade-f16c-49a0-9916-73b7a2316e61"},"source":["**Und wir haben unsere Autos gefunden!** Nun können wir einfach die Mitte des Autos berechnen und wir sind fertig.\n","\n","Wenn sich die Objekte welche wir suchen grundsätzlich in der Helligkeit unterscheiden ist es oft einfacher, ein Bild in ein Schwarz-Weiß-Bild umzuwandeln und zu sagen, alles was im Bild dunkel ist, das ist mein Objekt:"]},{"cell_type":"code","execution_count":null,"id":"8bc2bd78-47bf-433a-a119-6549eabab930","metadata":{"id":"8bc2bd78-47bf-433a-a119-6549eabab930"},"outputs":[],"source":["import requests\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","dogsImage = 'https://news.clemson.edu/wp-content/uploads/2022/03/german-shepherd-41731015-1130x636.jpg'\n","dogs = load_image_cv2(dogsImage)\n","\n","if dogs is not None:\n","    plt.subplot(1, 2, 1)\n","    plt.imshow(dogs)\n","    plt.subplot(1, 2, 2)\n","    plt.imshow(cv2.cvtColor(dogs, cv2.COLOR_BGR2GRAY) < 50, cmap='gray')\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"ec7942ca-0b6a-4a41-b2da-79a038e00d2a","metadata":{"id":"ec7942ca-0b6a-4a41-b2da-79a038e00d2a"},"outputs":[],"source":["# Aufgabe: Sucht euch seblst ein paar Bilder mit einem Objekt und versucht dieses automatisch zu finden!\n","import requests\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","link = 'todo'  # Hier müsst ihr ein Bild finden und den Link kopieren\n","bild = load_image_cv2(link)  # Hier wird das Bild geladen\n","\n","rot, grun, blau = bild[:, :, 0], bild[:, :, 1], bild[:, :, 2]  # Hier wird das Bild in seine rot, grün und blau Bestandteile getrennt\n","rot_auswahl = rot >< ?  # Hier müsst ihr den Rotanteil rausfiltern\n","grun_auswahl = grun >< ?  # Hier müsst ihr den Grünanteil rausfiltern\n","blau_auswahl = blau >< ?  # Hier müsst ihr den Blauanteil rausfiltern\n","kombiniert = rot_auswahl & grun_auswahl & blau_auswahl  # Hier werden die Filter kombiniert\n","\n","plt.subplot(1, 2, 1)\n","plt.imshow(bild)\n","plt.subplot(1, 2, 2)\n","plt.imshow(kombiniert, cmap='gray')\n","plt.show()\n"]},{"cell_type":"markdown","id":"59e3ca45-40bb-4417-b395-7e55ad846aa7","metadata":{"id":"59e3ca45-40bb-4417-b395-7e55ad846aa7"},"source":["### Erweiterte Objekterkennung\n","\n","Typischerweise werden Objekte nicht nur über die Farbe erkannt, auch wenn das häufig ein wichtiger Teil ist.\n","Ein weiterer wichtiger Schritt in der Objekterkennung ist die Kantenerkennung:"]},{"cell_type":"code","execution_count":null,"id":"625c3ed2-4c56-4d1c-b027-42287fcae0e6","metadata":{"id":"625c3ed2-4c56-4d1c-b027-42287fcae0e6"},"outputs":[],"source":["import cv2\n","import numpy as np\n","\n","cap = cv2.VideoCapture(0)\n","\n","while True:\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","\n","    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","\n","    # Anwendung der Kantenerkennung (X and Y Gradienten)\n","    sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)  # Horizontal\n","    sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)  # Vertikal\n","    sobel_combined = cv2.convertScaleAbs(cv2.addWeighted(sobel_x, 0.5, sobel_y, 0.5, 0))  # Kombinieren\n","\n","    cv2.imshow(\"Sobel Edge Detection\", sobel_combined)\n","\n","    if cv2.waitKey(1) & 0xFF == ord('q'):\n","        break\n","\n","cap.release()\n","cv2.destroyAllWindows()\n"]},{"cell_type":"markdown","id":"838fa48e-803a-44df-b37a-e3851bdd3bad","metadata":{"id":"838fa48e-803a-44df-b37a-e3851bdd3bad"},"source":["**Wie funktioniert das?**\n","\n","1. Wir brauchen ein Bild mit einem Objekt welches klare Kanten hat und einen Filter, mit welchem wir diese Kanten erkennen können.\n","2. Danach \"ziehen\" wir diesen Filter von links oben bis rechts unten über das Bild und multiplizieren jeden Pixel des Filters mit jedem Pixel des Bildes und summieren die Ergebnisse.\n","3. Das coole! Das ist alles was gemacht werden muss! Das ist ein einfacher Kantenerkenner, welche für eine Vielzahl an Objekterkennungssoftware verwendet wird!\n","\n","Anmerkung: Sogar moderne KI, welche auf Neuronale Netze aufbaut verwendet dieses einfache Filter Prinzip. Diese Neuronalen Netzwerke nennt man **Faltungsneuronale Netze**."]},{"cell_type":"code","execution_count":null,"id":"2adb288c-771a-44cd-b8e2-378f1b57d7c1","metadata":{"id":"2adb288c-771a-44cd-b8e2-378f1b57d7c1"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# 1) Erstellen eines 10x10 Bilds mit einem weißen 3x3 Block in der Mitte\n","bild = np.zeros((10, 10), dtype=np.uint8)  # Hintergrund\n","bild[3:6, 3:6] = 255  # Weißer Block\n","\n","# Definieren eines Weißen Objekts (2x2 Filter)\n","bildfilter = np.array([[0, 1],\n","                       [0, 1]])\n","\n","# Unwichtiger Code, zur Darstellung des Konzeptes:\n","fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n","\n","axes[0].imshow(bild, cmap='gray', interpolation='nearest', extent=[0, bild.shape[1], 0, bild.shape[0]])\n","axes[0].set_title(\"Originalbild\")\n","axes[1].imshow(bildfilter * 255, cmap='gray', interpolation='nearest', extent=[0, bildfilter.shape[1], 0, bildfilter.shape[0]])\n","axes[1].set_title(\"Filter\")\n","\n","for ax, img in zip(axes, [bild, bildfilter]):\n","    ax.set_xticks(np.arange(0, img.shape[1] + 1, 1))\n","    ax.set_yticks(np.arange(0, img.shape[0] + 1, 1))\n","    ax.grid(color=\"red\", linestyle=\"--\", linewidth=0.5)\n","    ax.tick_params(axis='both', which='both', bottom=False, left=False, labelbottom=False, labelleft=False)\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"9ca422d8-5e83-44ee-b49d-d6da2960d0bb","metadata":{"id":"9ca422d8-5e83-44ee-b49d-d6da2960d0bb"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Wie vorher ...\n","bild = np.zeros((10, 10), dtype=np.uint8)\n","bild[3:6, 3:6] = 255\n","bildfilter = np.array([[0, 1],\n","                       [0, 1]])\n","\n","# 2) Wir ziehen den Filter über das Bild\n","gefiltertesBild = np.zeros((9, 9))  # In dieses Bild speichern wir unser Ergebniss (Summe der Multiplikationen)\n","for y in range(9):  # Wir ziehen den Filter über alle Reihen ...\n","    for x in range(9):  # und über alle Spalten\n","        # Und berechnen hier die Summe der Multiplikation\n","        bildausschnitt = bild[y:y+2, x:x+2]\n","        gefiltertesBild[y, x] = np.sum(bildausschnitt * bildfilter)\n","\n","# Fertig! Nun können wir unser Ergebnis einfach ausgeben:\n","plt.imshow(gefiltertesBild, cmap='gray')\n","plt.show()\n"]},{"cell_type":"markdown","id":"9a6eb538-2174-4b94-bfac-1e3fd2b7054f","metadata":{"id":"9a6eb538-2174-4b94-bfac-1e3fd2b7054f"},"source":["**Dort wo das Bild am hellsten ist wurde die Kante gefunden. Warum gibt es einen grauen Rahmen? Könnt ihr einen Filter finden damit dieser Rahmen verschwindet?**\n","\n","## Live Gesichterkennung mittels Filter\n","\n","Wir können nun diese Idee nehmen und auf Gesichtserkennung anwenden:"]},{"cell_type":"code","execution_count":null,"id":"c555c90a-7e5d-4873-9e79-9afa7c24077d","metadata":{"id":"c555c90a-7e5d-4873-9e79-9afa7c24077d"},"outputs":[],"source":["import cv2\n","\n","# Hier laden wir vorgefertige Filter\n","face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n","\n","cap = cv2.VideoCapture(0)\n","\n","while True:\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","\n","    # Dann wandeln wir unser Bild in ein Schwarz-Weiß Bild um (die vorgefertigten Filter funktionieren nur auf schwarz-Weiß)\n","    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","\n","    # Und verwenden diese Filter für eine Geschichtserkennung\n","    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n","\n","    # Danach zeichnen wir noch ein grünes Rechteck um die gefundenen Gesichter\n","    for (x, y, w, h) in faces:\n","        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n","\n","    cv2.imshow(\"Face Detection\", frame)\n","\n","    if cv2.waitKey(1) & 0xFF == ord('q'):\n","        break\n","\n","cap.release()\n","cv2.destroyAllWindows()\n"]},{"cell_type":"markdown","id":"a81a2be7-9a18-4de6-ade5-2dece97db08b","metadata":{"id":"a81a2be7-9a18-4de6-ade5-2dece97db08b"},"source":["Frage: Welche Filter denkt ihr werden angewandt um Gesichter gut zu erkennen? Kann das zu Problemen führen?"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}